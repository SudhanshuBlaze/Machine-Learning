{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing in Python using spaCy library\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Here are the topics which we are going to cover:**\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Lemmatization\n",
    "\n",
    "Removing Punctuations and Stopwords\n",
    "\n",
    "Part of Speech Tagging\n",
    "\n",
    "Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embracing', 'and', 'analyzing', 'self', 'failures', '(', 'of', 'however', 'multitude', ')', 'is', 'a', 'virtue', 'of', 'nobelmen', '.']\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "\n",
    "#instantiating English module\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#sample\n",
    "x = \"Embracing and analyzing self failures (of however multitude) is a virtue of nobelmen.\"\n",
    "\n",
    "#creating doc object containing our token features\n",
    "doc = nlp(x)\n",
    "\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a sample code for Sentence tokenizing our text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embracing and analyzing self failures (of however multitude) is a virtue of nobelmen., And nobility is a treasure few possess.]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Creating the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Adding the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"Embracing and analyzing self failures (of however multitude) is a virtue of nobelmen. And nobility is a treasure few possess.\"\n",
    "\n",
    "#creating doc object carring our sentence tokens\n",
    "doc = nlp(text)\n",
    "\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token for token in doc.sents]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Running', 'run'), ('down', 'down'), ('the', 'the'), ('street', 'street'), ('with', 'with'), ('my', '-PRON-'), ('best', 'good'), ('buddy', 'buddy'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#sample\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "x = \"Running down the street with my best buddy.\"\n",
    "\n",
    "#creating doc object containing our token features\n",
    "doc = nlp(x)\n",
    "\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [(token.text,token.lemma_) for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out', 'only', 'eight', 'she', 'another', 'while', 'be', 'during', 'therefore', 'six', 'mine', 'since', 'in', 'show', 'sometime', 'thereupon', 'per', 'full', 'many', 'really', 'he', 'at', 'although', '’s', 'call', 'ca', 'to', 'when', '‘s', 'fifteen', 'both', '‘d', 'throughout', 'whose', 're', 'does', 'and', 'serious', 'by', 'is', 'else', 'was', 'hers', 'back', 'between', 'amongst', 'well', 'this', 'than', 'why', 'go', 'twelve', 'last', 'whither', 'before', 'due', 'yours', 'less', 'anyway', 'everything', 'indeed', 'yourselves', 'no', 'yet', 'anyhow', 'very', 'but', 'one', 'were', 'its', 'already', 'their', 'keep', 'though', 'done', 'those', '’re', 'part', \"'d\", 'except', 'been', 'his', 'within', 'make', 'whenever', 'towards', 'anyone', 'beside', 'becomes', 'move', 'namely', 'eleven', 'am', \"'re\", 'side', 'somewhere', 'some', 'various', 'used', 'me', 'often', 'empty', 'my', 'a', 'thence', 'ever', 'upon', \"'ve\", 'each', 'have', 'that', 'what', 'whereas', 'which', 'n‘t', 'thru', 'regarding', 'without', 'whence', 'whatever', 'n’t', 'after', 'mostly', 'always', 'toward', 'elsewhere', 'take', 'him', 'becoming', 'afterwards', 'himself', 'can', 'do', 'five', 'someone', 'fifty', 'together', 'ourselves', 'forty', 'rather', 'against', 'whoever', 'former', 'into', 'most', 'least', 'because', 'nowhere', 'whom', 'whereafter', 'latterly', 'nevertheless', 'nothing', 'all', 'made', 'say', 'us', \"'s\", '’m', 'herein', 'several', 'became', 'it', 'anywhere', 'front', 'same', 'as', 'must', 'ten', 'give', 'two', 'four', 'them', 'if', 'enough', 'become', 'themselves', 'the', 'doing', 'latter', 'nor', 'who', '’ll', 'among', 'either', 'could', '‘re', 'thereafter', 'something', 'too', 'might', 'not', \"'ll\", 'via', 'with', 'hundred', 'being', 'therein', 'put', 'an', 'every', 'whole', 'unless', \"n't\", 'see', 'about', 'third', 'three', 'few', 'below', 'there', 'itself', 'so', 'seem', 'seeming', 'still', 'sometimes', 'name', 'whereupon', 'you', 'however', 'these', 'along', '‘m', 'everyone', 'did', 'once', 'bottom', 'using', 'meanwhile', 'hence', 'cannot', 'everywhere', 'beforehand', 'also', 'are', 'down', 'where', 'should', 'whereby', 'your', 'twenty', 'none', 'hereafter', 'around', 'now', 'please', 'up', 'somehow', 'beyond', 'then', 'yourself', 'own', 'wherein', 'over', 'from', 'wherever', 'myself', 'again', '‘ll', 'behind', 'next', 'i', 'even', 'alone', 'of', 'ours', 'how', 'perhaps', 'get', 'seemed', 'may', 'her', 'quite', 'would', 'our', 'on', 'under', 'further', 'moreover', 'sixty', 'noone', 'above', 'besides', 'almost', 'or', 'more', \"'m\", 'hereupon', 'neither', 'across', 'herself', 'here', '‘ve', 'had', 'others', 'for', 'until', 'off', 'otherwise', 'thus', 'seems', '’ve', 'will', 'has', 'much', 'nobody', 'any', 'nine', 'through', 'such', 'never', 'whether', 'onto', 'formerly', 'just', 'they', '’d', 'hereby', 'other', 'top', 'we', 'anything', 'thereby', 'amount', 'first'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop = STOP_WORDS\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Running', 'down', 'the', 'street', 'with', 'my', 'best', 'buddy', '.']\n",
      "['Running', 'street', 'best', 'buddy', '.']\n"
     ]
    }
   ],
   "source": [
    "#sample\n",
    "x = \"Running down the street with my best buddy.\"\n",
    "\n",
    "#creation of doc object containing our token features\n",
    "\n",
    "doc = nlp(x)\n",
    "\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "#Creating and updating our list of filtered tokens using list comprehension \n",
    "\n",
    "filtered = [token.text for token in doc if token.is_stop == False]\n",
    "\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "\n",
    "You can compare the above two lists and notice words such as down,the,with and my have been removed.Now, similarly, we can also remove punctuation from our text as well using \"isalpha\" method of string objects and using list comprehensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BLIMEY', '!', '!', 'Such', 'an', 'exhausting', 'day', ',', 'I', 'ca', \"n't\", 'even', 'describe', '.']\n",
      "['BLIMEY', 'exhausting', 'day', 'describe'] \n",
      "\n",
      "BLIMEY exhausting day describe\n"
     ]
    }
   ],
   "source": [
    "#sample \n",
    "x = \"BLIMEY!! Such an exhausting day, I can't even describe.\"\n",
    "\n",
    "#creation of doc object containing our token features\n",
    "doc = nlp(x)\n",
    "\n",
    "#Unfiltered tokens \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "\n",
    "#Filtering our tokens\n",
    "filtered = [token.text for token in doc if token.is_stop == False and       \n",
    "token.text.isalpha() == True]\n",
    "\n",
    "print(filtered,\"\\n\")\n",
    "print(\" \".join(filtered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging (POS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Robin', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('astute', 'NOUN'), ('programmer', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "#sample\n",
    "x = \"Robin is an astute programmer\"\n",
    "\n",
    "#Creating doc object\n",
    "doc = nlp(x)\n",
    "\n",
    "#Extracting POS\n",
    "pos = [(token.text,token.pos_) for token in doc]\n",
    "print (pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(India, 'GPE', 384), (as much as 3, 'CARDINAL', 397), ($39 billion, 'MONEY', 394), (Asia, 'LOC', 385), (third, 'ORDINAL', 396)]\n"
     ]
    }
   ],
   "source": [
    "#sample article\n",
    "x = u\"\"\" India is considering a proposal to guarantee as much as 3  \n",
    "trillion rupees ($39 billion) of loans to small businesses as part of a plan to \n",
    "restart Asia's third-largest economy, which is reeling under the impact of a 40-\n",
    "day lockdown, people with knowledge of the matter said.\"\"\"\n",
    "\n",
    "#creating doc object\n",
    "bloomberg= nlp(x)\n",
    "\n",
    "\n",
    "#extracting entities \n",
    "entities=[(i, i.label_, i.label) for i in bloomberg.ents]\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing all these techniques on our email-spam detection data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/sudhanshukumar/Documents/Development/Machine Learning/0 csv files/SMSSpamCollection.csv\",sep=\"\\t\",names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(message):\n",
    "    doc=nlp(message)\n",
    "    final=[token.lemma_ for token in doc if token.is_stop== False and token.text.isalpha()== True ]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embrace', 'analyze', 'self', 'failure', 'multitude', 'virtue', 'nobelman', 'nobility', 'treasure', 'possess'] \n",
      "\n",
      "embrace analyze self failure multitude virtue nobelman nobility treasure possess\n"
     ]
    }
   ],
   "source": [
    "text = \"Embracing AND analyzing self failures (of however multitude) is a virtue of nobelmen. And nobility is a treasure few possess.\"\n",
    "print(text_process(text),\"\\n\")\n",
    "print(\" \".join(text_process(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [jurong, point, crazy, available, bugis, n, gr...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, wkly, comp, win, FA, Cup, final,...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message\"].head(3).apply(text_process)  #apply func works on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect & remove empty strings\n",
    "Technically, we're dealing with \"whitespace only\" strings. If the original .tsv file had contained empty strings, pandas **.read_csv()** would have assigned NaN values to those cells by default.\n",
    "\n",
    "In order to detect these strings we need to iterate over each row in the DataFrame. The **.itertuples()** pandas method is a good tool for this as it provides access to every field. For brevity we'll assign the names `i`, `lb` and `rv` to the `index`, `label` and `review` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,msg in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(msg)==str:            # avoid NaN values\n",
    "        if msg.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(analyzer=text_process)\n",
    "\n",
    "X=tfidf.fit_transform(df[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9802690582959641"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=LinearSVC()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838565022421525"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without analyzer\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "X=tfidf.fit_transform(df[\"message\"])\n",
    "y=df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf=LinearSVC()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9901345291479821"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "\n",
    "X=cv.fit_transform(df[\"message\"])\n",
    "y=df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf=LinearSVC()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
