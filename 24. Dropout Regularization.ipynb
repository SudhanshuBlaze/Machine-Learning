{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting are common phenomena in the field of machine learning and the techniques used to tackle overfitting problem is called **regularization**. In deep learning, dropout regularization is used to randomly drop neurons from hidden layers and this helps with generalization. We will implement **artificial neural network** for binary classification problem and see how using dropout layer can increase the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Set Information**:\n",
    "\n",
    "The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file \"sonar.rocks\" contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock. \n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp. \n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"./0 resources/sonar.csv\",header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    111\n",
       "R     97\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[60].value_counts()   # M stands for Mines and R stands for Rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(60,axis=1)\n",
    "#OHE   -> because Deep Learning Model won't work with string data\n",
    "y=pd.get_dummies(df[60],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R\n",
       "0    111\n",
       "1     97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()  # R->1 & M->0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((139, 60), (139, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7053 - accuracy: 0.4748\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.6187\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.7194\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.7050\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.7050\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5930 - accuracy: 0.7554\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5665 - accuracy: 0.7626\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7842\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.8345\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7770\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.7626\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8633\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8417\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3613 - accuracy: 0.8633\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3375 - accuracy: 0.8561\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8705\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8849\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.2960 - accuracy: 0.8921\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.8993\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8777\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2342 - accuracy: 0.9281\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9424\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.2078 - accuracy: 0.9137\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9568\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1793 - accuracy: 0.9424\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9712\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9712\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9640\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9712\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9568\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1286 - accuracy: 0.9568\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9640\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9856\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9640\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9856\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9928\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9784\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0683 - accuracy: 0.9784\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9928\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.7755e-04 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.4285e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.2752e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9.5047e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.8159e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.8376e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.8036e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.7446e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 8.0362e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 7.9631e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.9424e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.5844e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.4213e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.1849e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.3037e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7.0747e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.7428e-04 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.6277e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.4818e-04 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.3039e-04 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.3687e-04 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6.0737e-04 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.8940e-04 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.8049e-04 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.7479e-04 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.5933e-04 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.5281e-04 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.6301e-04 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.6530e-04 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.2272e-04 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.3821e-04 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0257e-04 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9072e-04 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.8492e-04 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.7605e-04 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.6508e-04 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.5208e-04 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.5378e-04 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.7275e-04 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2802e-04 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.2074e-04 - accuracy: 1.0000\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 4.1216e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.1554e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.0518e-04 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.9453e-04 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.8400e-04 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.7490e-04 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.6644e-04 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 3.6455e-04 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5546e-04 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4897e-04 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4461e-04 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.3424e-04 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.3517e-04 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.2080e-04 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.2446e-04 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.1689e-04 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.0683e-04 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.0077e-04 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.9990e-04 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.9064e-04 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.9106e-04 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.9787e-04 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.7359e-04 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.7082e-04 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.7508e-04 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.6825e-04 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.6201e-04 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.4892e-04 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.4793e-04 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.4670e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.3652e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.4197e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.2882e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.3000e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.2226e-04 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.2347e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.1649e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.1693e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 2.2088e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf99568290>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building ANN model\n",
    "#input_shape-> builds the input layer of same number of neurons as the input features, 50 is the hidden layer\n",
    "\n",
    "model1=keras.Sequential([\n",
    "    keras.layers.Dense(50, input_dim=60,activation=\"relu\"),  #input_shape=(60,)\n",
    "    keras.layers.Dense(30,activation=\"relu\"),\n",
    "    keras.layers.Dense(15,activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#batch_size: Number of samples per gradient update. \n",
    "#Mini-Batch Gradient Descent. In each iteration, 8 samples are feeded calculate the error and do a back propagation.\n",
    "model1.fit(X_train,y_train,epochs=200,batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model got the training-set accuracy of 100% but testing it against the test data , we are getting the accuracy of 85% only, this is due to Overfitting, I will introduce the **drop-out layer to drop neurons in between the hidden layers**, this will lead to neurons not depending on any one input, as it may be dropped at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6306 - accuracy: 0.8551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6305652856826782, 0.8550724387168884]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35  3]\n",
      " [ 7 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.88        38\n",
      "           1       0.89      0.77      0.83        31\n",
      "\n",
      "    accuracy                           0.86        69\n",
      "   macro avg       0.86      0.85      0.85        69\n",
      "weighted avg       0.86      0.86      0.85        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model1.predict_classes(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing Dropout layers in between Hidden layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6987 - accuracy: 0.5252\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7001 - accuracy: 0.5036\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7208 - accuracy: 0.4317\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7274 - accuracy: 0.5180\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5612\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5612\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5540\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5324\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6973 - accuracy: 0.5252\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6706 - accuracy: 0.5683\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6772 - accuracy: 0.5755\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6957 - accuracy: 0.5252\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.6115\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6602 - accuracy: 0.6259\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.5324\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7065 - accuracy: 0.5036\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6978 - accuracy: 0.5108\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.5468\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6805 - accuracy: 0.5612\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.5252\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.5180\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.5612\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.5612\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.5971\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.6187\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.5827\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.5755\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6410 - accuracy: 0.6763\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.6187\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.6763\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.6691\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6628 - accuracy: 0.5827\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.6259\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.5899\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6231 - accuracy: 0.7050\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.5899\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.6115\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6154 - accuracy: 0.6619\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.6403\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.6906\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.6763\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.6906\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6086 - accuracy: 0.6043\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.6906\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.6475\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.7266\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.6978\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.6978\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.6835\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7266\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7122\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5520 - accuracy: 0.7122\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7626\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5446 - accuracy: 0.7122\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5236 - accuracy: 0.8129\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6012 - accuracy: 0.7194\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7338\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7050\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.7266\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7266\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5365 - accuracy: 0.7410\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7698\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7554\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7914\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7266\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7482\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.6978\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7626\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.8633\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7770\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7842\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7770\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.7554\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.7770\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.7698\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.7482\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4193 - accuracy: 0.7914\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8129\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4229 - accuracy: 0.8058\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8201\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8417\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.7698\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.8058\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.7986\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8705\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8273\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8273\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8561\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8561\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8129\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8345\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8849\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8705\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.9065\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8489\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8561\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.8993\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8633\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8633\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8633\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8705\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8345\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3159 - accuracy: 0.8633\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8705\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3575 - accuracy: 0.8345\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8849\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9065\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8921\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8633\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2828 - accuracy: 0.8777\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.9065\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.9065\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8921\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.8921\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8777\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.9281\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.8849\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.8777\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8705\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3119 - accuracy: 0.8417\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2883 - accuracy: 0.8705\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8561\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.8849\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9281\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8489\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.2731 - accuracy: 0.9065\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.2911 - accuracy: 0.8777\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8489\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.8777\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.8705\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8921\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.9209\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2380 - accuracy: 0.8849\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.8921\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.2576 - accuracy: 0.8849\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.2156 - accuracy: 0.9065\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.2093 - accuracy: 0.9137\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.2351 - accuracy: 0.9065\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.2396 - accuracy: 0.9065\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3345 - accuracy: 0.8705\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.2327 - accuracy: 0.8921\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.9065\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9065\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2259 - accuracy: 0.9209\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9496\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2589 - accuracy: 0.8705\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9209\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9281\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1588 - accuracy: 0.9496\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1858 - accuracy: 0.9353\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1755 - accuracy: 0.9496\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2451 - accuracy: 0.8921\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9353\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9209\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9353\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9137\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9424\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9209\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1494 - accuracy: 0.9496\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9496\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.9568\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9353\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9640\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9137\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1794 - accuracy: 0.9353\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9209\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1909 - accuracy: 0.9209\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9281\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9496\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1637 - accuracy: 0.9281\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.9496\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1519 - accuracy: 0.9424\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.8993\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9353\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9496\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9568\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.9209\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9353\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.9568\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.9209\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9137\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9281\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9496\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9137\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9137\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 0.9353\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.8993\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9424\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9281\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1586 - accuracy: 0.9424\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1686 - accuracy: 0.9137\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1959 - accuracy: 0.8993\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9065\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9137\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9568\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.9281\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1776 - accuracy: 0.9209\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1486 - accuracy: 0.9496\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1642 - accuracy: 0.9281\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.9353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf9aabb750>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=keras.Sequential([\n",
    "    keras.layers.Dense(50, input_dim=60,activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),  #drops 50% data,  Dropout layers are introduced between hidden layers\n",
    "    keras.layers.Dense(30,activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),  #drops 50% data\n",
    "    keras.layers.Dense(15,activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),  #drops 20% data\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model2.fit(X_train,y_train,epochs=200,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.8986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.448188453912735, 0.8985507488250732]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  2]\n",
      " [ 5 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91        38\n",
      "           1       0.93      0.84      0.88        31\n",
      "\n",
      "    accuracy                           0.90        69\n",
      "   macro avg       0.90      0.89      0.90        69\n",
      "weighted avg       0.90      0.90      0.90        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model2.predict_classes(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improved f1 score and Accuracy**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
